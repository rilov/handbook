{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - LangChain Essentials: Building Blocks of AI Applications\n",
    "\n",
    "Practice the core building blocks: chat models, prompting, structured outputs, and chaining.\n",
    "\n",
    "## ðŸ“¦ Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain langchain-openai pydantic python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "\n",
    "print(\"âœ… Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– Chat Models\n",
    "\n",
    "### Basic Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create chat model\n",
    "chat = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.7)\n",
    "\n",
    "response = chat.invoke(\"Explain quantum physics in simple terms\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Different Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature = 0 (deterministic)\n",
    "chat_precise = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "print(\"Temperature 0:\")\n",
    "print(chat_precise.invoke(\"Write a poem about AI\").content)\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Temperature = 1.5 (creative)\n",
    "chat_creative = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=1.5)\n",
    "print(\"Temperature 1.5:\")\n",
    "print(chat_creative.invoke(\"Write a poem about AI\").content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’¬ Prompting Patterns\n",
    "\n",
    "### Pattern 1: Role-Based Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful teacher explaining to a 10-year-old.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | chat\n",
    "response = chain.invoke({\"question\": \"What is DNA?\"})\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Few-Shot Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a sentiment analyzer.\"),\n",
    "    (\"user\", \"Review: This movie was amazing! â†’ Sentiment: Positive\"),\n",
    "    (\"user\", \"Review: Terrible waste of time â†’ Sentiment: Negative\"),\n",
    "    (\"user\", \"Review: {review} â†’ Sentiment:\")\n",
    "])\n",
    "\n",
    "chain = few_shot_prompt | chat\n",
    "result = chain.invoke({\"review\": \"It was okay, nothing special\"})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Structured Outputs\n",
    "\n",
    "### Using Pydantic for Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"Person's full name\")\n",
    "    age: int = Field(description=\"Person's age\")\n",
    "    occupation: str = Field(description=\"Person's job\")\n",
    "\n",
    "# Create structured LLM\n",
    "structured_llm = chat.with_structured_output(Person)\n",
    "\n",
    "# Extract structured data\n",
    "text = \"John Smith is a 35-year-old software engineer\"\n",
    "person = structured_llm.invoke(f\"Extract person info from: {text}\")\n",
    "\n",
    "print(f\"Name: {person.name}\")\n",
    "print(f\"Age: {person.age}\")\n",
    "print(f\"Occupation: {person.occupation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ”— Chaining with LCEL\n",
    "\n",
    "### Simple Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "model = ChatOpenAI()\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Chain them together\n",
    "chain = prompt | model | parser\n",
    "\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Step Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Generate poem\n",
    "poem_prompt = ChatPromptTemplate.from_template(\"Write a 2-line poem about {topic}\")\n",
    "\n",
    "# Step 2: Translate\n",
    "translate_prompt = ChatPromptTemplate.from_template(\"Translate to Spanish: {poem}\")\n",
    "\n",
    "# Combine\n",
    "chain = (\n",
    "    {\"poem\": poem_prompt | model | StrOutputParser()}\n",
    "    | translate_prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "result = chain.invoke({\"topic\": \"ocean\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ§ª Practice: Complete Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableParallel\n",
    "\n",
    "# Define multiple analysis chains\n",
    "summary_prompt = ChatPromptTemplate.from_template(\"Summarize in one sentence: {text}\")\n",
    "sentiment_prompt = ChatPromptTemplate.from_template(\"What's the sentiment (positive/negative/neutral): {text}\")\n",
    "keywords_prompt = ChatPromptTemplate.from_template(\"Extract 3 keywords from: {text}\")\n",
    "\n",
    "# Run in parallel\n",
    "parallel_chain = RunnableParallel(\n",
    "    summary=summary_prompt | chat | StrOutputParser(),\n",
    "    sentiment=sentiment_prompt | chat | StrOutputParser(),\n",
    "    keywords=keywords_prompt | chat | StrOutputParser()\n",
    ")\n",
    "\n",
    "text = \"\"\"AI is transforming technology. Machine learning enables computers to learn \n",
    "from data without explicit programming. The future looks exciting!\"\"\"\n",
    "\n",
    "result = parallel_chain.invoke({\"text\": text})\n",
    "print(\"Summary:\", result[\"summary\"])\n",
    "print(\"Sentiment:\", result[\"sentiment\"])\n",
    "print(\"Keywords:\", result[\"keywords\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Summary\n",
    "\n",
    "You've learned:\n",
    "- Chat models and temperature control\n",
    "- Prompting patterns (role-based, few-shot)\n",
    "- Structured outputs with Pydantic\n",
    "- Chaining with LCEL\n",
    "\n",
    "**Next:** Part 3 - Tool Calling"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
